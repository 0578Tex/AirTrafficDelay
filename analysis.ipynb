{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data_prep import DataPreparation, filtering_data\n",
    "from lstm import LSTMModelTrainerAttention\n",
    "from rf import RandomForestTrainer\n",
    "from lgbm import LightGBMTrainer\n",
    "from catboost_trainer import CatBoostTrainer\n",
    "import os\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from analysis import plot_correlation_heatmap, get_ETO_DEP\n",
    "from lstm import LSTMModelTrainerAttention, calculate_and_plot_errors#, LSTMModelTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import tqdm\n",
    "from lstm import LSTMRollingForecaster\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import extract_flight_details\n",
    "from lstm import LSTMModelTrainerAttention, LSTMRollingForecaster\n",
    "from rf import RandomForestTrainer\n",
    "from lgbm import LightGBMTrainer\n",
    "from catboost_trainer import CatBoostTrainer\n",
    "from transformer import TransformerModelTrainer\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= r'C:\\Users\\iLabs_6\\Documents\\Tex\\realtimetest3'\n",
    "modelname = 'newbaseline_atot2'\n",
    "\n",
    "with  open(os.path.join(output, modelname), 'rb') as f:  \n",
    "    extended_df = pickle.load(f)\n",
    "\n",
    "extended_df['cap_DEP'].fillna(extended_df['cap_DEP'].mean(), inplace=True)\n",
    "extended_df['cap_DES'].fillna(extended_df['cap_DES'].mean(), inplace=True)\n",
    "extended_df = extended_df.loc[:, ~extended_df.columns.str.contains('t_to_eobt', case=False)]\n",
    "extended_df = extended_df.loc[:, ~extended_df.columns.str.contains('t_to_atot', case=False)]\n",
    "extended_df = extended_df.drop_duplicates()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "X, y, colnames = filtering_data(extended_df, airport ='EHAM', save=True)\n",
    "X = X.drop(['ADES_EHAM', 'ADESLong', 'ADESLat', 'cbasentry'], axis =1)\n",
    "\n",
    "modelname = 'newbaseline_etot2'\n",
    "\n",
    "\n",
    "with  open(os.path.join(output, modelname), 'rb') as f:  \n",
    "    extended_real= pickle.load(f)\n",
    "extended_real = extended_real.loc[:, ~extended_real.columns.str.contains('t_to_eobt', case=False)]\n",
    "extended_real = extended_real.loc[:, ~extended_real.columns.str.contains('t_to_atot', case=False)]\n",
    "extended_real = extended_real.drop_duplicates()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "X_r, y_r, colnames = filtering_data(extended_real, airport ='EHAM', save=False)\n",
    "X_r = X_r.drop(['ADES_EHAM', 'ADESLong', 'ADESLat', 'cbasentry'], axis =1)\n",
    "\n",
    "modelpath= r\"C:\\Users\\iLabs_6\\Documents\\Tex\\AirTrafficDelays\\LSTM_Models\"\n",
    "\n",
    "modelname = 'newbaseline-s6'\n",
    "\n",
    "with  open(os.path.join(modelpath, modelname), 'rb') as f:  \n",
    "    best_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep = DataPreparation()\n",
    "\n",
    "\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, time_horizons, cbaslabels = data_prep.fit_transform_data(X, y)\n",
    "X_real, ETOT_horizons, cbaslabels = data_prep.transform_data(X_r)\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_prep.fit_transform_data(X, y, split_ratio=0.8, mode='rf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "abs_ap = []\n",
    "ap = []\n",
    "lstm_se = []\n",
    "lstm_y_true =[]\n",
    "lstm_y_pred = []\n",
    "target_length = 61  # \n",
    "start_idx, end_idx =int( 0.8*len(y_r)), -1  #all!\n",
    "\n",
    "abs_error_dict = {}\n",
    "error_dict = {}\n",
    "\n",
    "e_adep = defaultdict(list)\n",
    "e_dist =  defaultdict(list)\n",
    "e_wd =  defaultdict(list)\n",
    "e_time =  defaultdict(list)\n",
    "adeplonglist = defaultdict(list)\n",
    "adeplatlist = defaultdict(list)\n",
    "adeplatlonlist = defaultdict(list)\n",
    "actypelist = defaultdict(list)\n",
    "flighttypelist = defaultdict(list)\n",
    "\n",
    "eto_dict = {}\n",
    "# Loop through the specified range in `y_r`\n",
    "for fnr in tqdm(np.where((y_r <= 130))[0][start_idx:end_idx]):\n",
    "    # Create rolling forecaster instance\n",
    "    recursive = LSTMRollingForecaster(best_model, data_prep, X_real[fnr], ETOT_horizons)\n",
    "    pred = recursive.rolling_forecast()\n",
    "    # Calculate absolute error per timestep\n",
    "\n",
    "    ### WR ATOT\n",
    "    absolute_error_per_timestep = np.abs(pred[-target_length:] - y_r[fnr])\n",
    "    error_per_timestep = (pred[-target_length:] - y_r[fnr])\n",
    "    squared_error_per_timestep = (pred[-target_length:] - y_r[fnr]) ** 2\n",
    "\n",
    "    # Ensure each error array has `target_length` elements by padding with NaNs if needed\n",
    "    if len(absolute_error_per_timestep) < target_length:\n",
    "        pad_length = target_length - len(absolute_error_per_timestep)\n",
    "\n",
    "        padded_abs_error = np.pad(absolute_error_per_timestep, (0, target_length - len(absolute_error_per_timestep)), constant_values=np.nan)\n",
    "        padded_error = np.pad(error_per_timestep, (0, target_length - len(error_per_timestep)), constant_values=np.nan)\n",
    "        padded_sq_error = np.pad(squared_error_per_timestep, (0, pad_length), constant_values=np.nan)\n",
    "\n",
    "        abs_ap.append(padded_abs_error)\n",
    "        ap.append(padded_error)\n",
    "        lstm_se.append(padded_sq_error)\n",
    "\n",
    "    else:\n",
    "        abs_ap.append(absolute_error_per_timestep)\n",
    "        ap.append(error_per_timestep)\n",
    "        lstm_se.append(squared_error_per_timestep)\n",
    "\n",
    "    y_true = y_r[fnr] * np.ones(target_length)  # Repeat the scalar true value\n",
    "    y_pred = pred[-target_length:]\n",
    "\n",
    "    # Handle cases where there might not be enough predicted values\n",
    "    if len(y_pred) < target_length:\n",
    "        pad_length = target_length - len(y_pred)\n",
    "        y_true_padded = y_true\n",
    "        y_pred_padded = np.pad(y_pred, (0, pad_length), constant_values=np.nan)\n",
    "    else:\n",
    "        y_true_padded = y_true\n",
    "        y_pred_padded = y_pred\n",
    "\n",
    "    # Append the padded true and predicted values\n",
    "    lstm_y_true.append(y_true_padded)\n",
    "    lstm_y_pred.append(y_pred_padded)\n",
    "\n",
    "    ### REST (CBAS)\n",
    "\n",
    "    absolute_error_per_timestep = np.abs(pred[-target_length:] - y_r[fnr])[::-1]\n",
    "    error_per_timestep = (pred[-target_length:] - y_r[fnr])[::-1]\n",
    "    clabel = [x.astype('timedelta64[s]').astype(int) / 60 for x in cbaslabels[fnr] if not np.isnat(x)][-target_length:][::-1]\n",
    "\n",
    "    if len(absolute_error_per_timestep) > len(clabel):\n",
    "        absolute_error_per_timestep = absolute_error_per_timestep[-len(clabel):]\n",
    "        error_per_timestep = error_per_timestep[-len(clabel):]\n",
    "\n",
    "\n",
    "    inv  = data_prep.inverse_transform_single_flight(X_real[fnr])\n",
    "    info = extract_flight_details(inv)\n",
    "    etocols =inv[[c for c in inv.columns if 'etodep' in c]]\n",
    "\n",
    "    eto_error = (etocols.iloc[0].tolist() - y_r[fnr])\n",
    "    eto_e = eto_error[~np.isnan(eto_error)][::-1]\n",
    "    \n",
    "    # Loop over the filtered `clabel` and `absolute_error_per_timestep`\n",
    "    for i in range(len(absolute_error_per_timestep)):\n",
    "        time_to_cbas = np.round(clabel[i], 0)\n",
    "        if time_to_cbas not in abs_error_dict:\n",
    "            error_dict[time_to_cbas] = []\n",
    "            abs_error_dict[time_to_cbas] = []\n",
    "            eto_dict[time_to_cbas]=[]\n",
    "        abs_error_dict[time_to_cbas].append(absolute_error_per_timestep[i])\n",
    "        error_dict[time_to_cbas].append(error_per_timestep[i])\n",
    "        try:\n",
    "\n",
    "            eto_dict[time_to_cbas].append(eto_e[i])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    ### FOR ATOT GRAPTH ANA:LYYSY\n",
    "\n",
    "    absolute_error_per_timestep = np.abs(pred[-target_length:] - y_r[fnr])\n",
    "    error_per_timestep = pred[-target_length:] - y_r[fnr]\n",
    "\n",
    "    # Ensure each error array has `target_length` elements by padding with NaNs if needed\n",
    "    if len(absolute_error_per_timestep) < target_length:\n",
    "        absolute_error_per_timestep = np.pad(absolute_error_per_timestep, (0, target_length - len(absolute_error_per_timestep)), constant_values=np.nan)\n",
    "\n",
    "    e_adep[info['adep']].append(absolute_error_per_timestep[-61:])\n",
    "    e_dist[info['distance']].append(absolute_error_per_timestep[-61:])\n",
    "    e_wd[info['day']].append(absolute_error_per_timestep[-61:])\n",
    "    e_time[info['ETOT']].append(absolute_error_per_timestep[-61:])\n",
    "    adeplonglist[info['longitude']].append(absolute_error_per_timestep[-61:])\n",
    "    adeplatlist[info['latitude']].append(absolute_error_per_timestep[-61:])\n",
    "    adeplatlonlist[(info['latitude'], info['longitude'])].append(absolute_error_per_timestep)\n",
    "    actypelist[info['actype']].append(absolute_error_per_timestep[-61:])\n",
    "    flighttypelist[info['flighttype']].append(absolute_error_per_timestep[-61:])\n",
    "\n",
    "\n",
    "\n",
    "lstm_se = np.array(lstm_se)           # Shape: (num_samples, target_length)\n",
    "lstm_rmse_per_timestep = np.sqrt(np.nanmean(lstm_se, axis=0))\n",
    "overall_rmse = np.sqrt(np.nanmean(lstm_se))\n",
    "\n",
    "\n",
    "# Bucket the errors into 5-minute intervals\n",
    "bucketed_abs_errors = defaultdict(list)\n",
    "for time_in_minutes, errors in abs_error_dict.items():\n",
    "    bucket = int(time_in_minutes // 5) * 5   # Group into 5-minute buckets\n",
    "    bucketed_abs_errors[bucket].extend(errors)   # Aggregate errors within the bucket\n",
    "\n",
    "bucketed_errors = defaultdict(list)\n",
    "for time_in_minutes, errors in error_dict.items():\n",
    "    bucket = int(time_in_minutes // 5) * 5   # Group into 5-minute buckets\n",
    "    bucketed_errors[bucket].extend(errors)   # Aggregate errors within the bucket\n",
    "\n",
    "eto_bucket_errors = defaultdict(list)\n",
    "for time_in_minutes, errors in eto_dict.items():\n",
    "    bucket = int(time_in_minutes // 5) * 5   # Group into 5-minute buckets\n",
    "    eto_bucket_errors[bucket].extend(errors)   # Aggregate errors within the bucket\n",
    "\n",
    "# Compute the mean and standard deviation of error for each bucket\n",
    "mean_abs_errors_by_bucket = {bucket: np.nanmean(errors) for bucket, errors in bucketed_abs_errors.items()}\n",
    "mean_errors_by_bucket = {bucket: np.nanmean(errors) for bucket, errors in bucketed_errors.items()}\n",
    "std_errors_by_bucket = {bucket: np.nanstd(errors) for bucket, errors in bucketed_errors.items()}\n",
    "eto_errors_by_bucket = {bucket: np.nanstd(errors) for bucket, errors in eto_bucket_errors.items()}\n",
    "\n",
    "# Sort the buckets\n",
    "sorted_abs_buckets = sorted(mean_abs_errors_by_bucket.items())\n",
    "sorted_buckets = sorted(mean_errors_by_bucket.items())\n",
    "sorted_std_buckets = sorted(std_errors_by_bucket.items())\n",
    "sorted_eto_buckets = sorted(eto_errors_by_bucket.items())\n",
    "\n",
    "# Extract sorted times and mean errors for plotting\n",
    "abs_times = [item[0] for item in sorted_abs_buckets]\n",
    "abs_errors = [item[1] for item in sorted_abs_buckets]\n",
    "times = [item[0] for item in sorted_buckets]\n",
    "errors = [item[1] for item in sorted_buckets]\n",
    "std_errors = [item[1] for item in sorted_std_buckets]\n",
    "eto_errors = [item[1] for item in sorted_eto_buckets]\n",
    "\n",
    "# Plot Mean Absolute Prediction Error\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(abs_times[:-10], abs_errors[:-10], marker='o', linestyle='-')\n",
    "plt.title(\"Mean Absolute Prediction Error vs Time to CBAS Entry (bucketed in 5-minute intervals)\")\n",
    "plt.xlabel(\"Time to CBAS Entry (minutes)\")\n",
    "plt.ylabel(\"Mean Absolute Prediction Error (minutes)\")\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "time_values = [item[0] for item in sorted_buckets]\n",
    "xt = [x for x in range(0,320,20)][::-1]\n",
    "print(f'{xt=}')\n",
    "plt.xticks(xt, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(times[:-10], eto_errors[:-10], marker='o', linestyle='-')\n",
    "plt.title(\"Mean Absolute Prediction Error vs Time to CBAS Entry (bucketed in 5-minute intervals)\")\n",
    "plt.xlabel(\"Time to CBAS Entry (minutes)\")\n",
    "plt.ylabel(\"Mean Absolute Prediction Error (minutes)\")\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Mean Prediction Error with Standard Deviation as Fill\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(times[:-10], errors[:-10], marker='o', linestyle='-', label='Mean Error')\n",
    "plt.fill_between(\n",
    "    times[:-10],\n",
    "    [e - s for e, s in zip(errors[:-10], std_errors[:-10])],\n",
    "    [e + s for e, s in zip(errors[:-10], std_errors[:-10])],\n",
    "    color='b',\n",
    "    alpha=0.2,\n",
    "    label='±1 Std Dev'\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Mean Prediction Error vs Time to CBAS Entry (bucketed in 5-minute intervals)\")\n",
    "plt.xlabel(\"Time to CBAS Entry (minutes)\")\n",
    "plt.ylabel(\"Mean Prediction Error (minutes)\")\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "lstm_y_true = np.array(lstm_y_true)  # Shape: (num_samples, target_length)\n",
    "lstm_y_pred = np.array(lstm_y_pred)  # Shape: (num_samples, target_length)\n",
    "\n",
    "# Flatten arrays for overall R² calculation\n",
    "flat_y_true = lstm_y_true.flatten()\n",
    "flat_y_pred = lstm_y_pred.flatten()\n",
    "valid_mask = ~np.isnan(flat_y_true) & ~np.isnan(flat_y_pred)\n",
    "\n",
    "if np.any(valid_mask):\n",
    "    overall_r2 = r2_score(flat_y_true[valid_mask], flat_y_pred[valid_mask])\n",
    "else:\n",
    "    overall_r2 = np.nan\n",
    "\n",
    "\n",
    "overall_mae = np.nanmean(abs_ap[-61:])\n",
    "overall_rmse = np.sqrt(np.nanmean(lstm_se[-61:]))\n",
    "\n",
    "overall_std = np.nanstd(abs_ap[-61:])\n",
    "\n",
    "\n",
    "\n",
    "lstm_r2_per_timestep = []\n",
    "for t in range(target_length):\n",
    "    y_true_t = lstm_y_true[:, t]\n",
    "    y_pred_t = lstm_y_pred[:, t]\n",
    "    mask_t = ~np.isnan(y_true_t) & ~np.isnan(y_pred_t)\n",
    "    if np.sum(mask_t) > 1:\n",
    "        r2_t = r2_score(y_true_t[mask_t], y_pred_t[mask_t])\n",
    "    else:\n",
    "        r2_t = np.nan\n",
    "    lstm_r2_per_timestep.append(r2_t)\n",
    "rf_r2_per_timestep = np.array(lstm_r2_per_timestep)\n",
    "print(f'{rf_r2_per_timestep=}')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ETOT_horizons[:target_length], lstm_r2_per_timestep, marker='s', color='green', label='R²')\n",
    "plt.xlabel('Timestep')\n",
    "plt.xticks(ETOT_horizons[:target_length], rotation='vertical')\n",
    "plt.ylabel('R²')\n",
    "plt.title('Random Forest: R² for Each Timestep')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Raw True Values (y_true):\", lstm_y_true)\n",
    "print(\"Raw Predictions (y_pred):\", lstm_y_pred)\n",
    "print(f\"Overall MAE: {overall_mae:.4f}\")\n",
    "print(f\"Overall RMSE: {overall_rmse:.4f}\")\n",
    "print(f\"Overall R²: {overall_r2:.4f}\")\n",
    "print(f\"Overall Std Deviation of MAE: {overall_std:.4f}\")\n",
    "  \n",
    "# Calculate mean absolute error (MAE) per timestep, ignoring NaNs in the calculation\n",
    "\n",
    "print(f'{lstm_rmse_per_timestep=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "day_mapping = {\n",
    "    'day_of_week_0': 'Monday',\n",
    "    'day_of_week_1': 'Tuesday',\n",
    "    'day_of_week_2': 'Wednesday',\n",
    "    'day_of_week_3': 'Thursday',\n",
    "    'day_of_week_4': 'Friday',\n",
    "    'day_of_week_5': 'Saturday',\n",
    "    'day_of_week_6': 'Sunday'\n",
    "}\n",
    "\n",
    "# Function to pad or truncate error lists for consistency\n",
    "def pad_errors(errors, target_length=61):\n",
    "    padded_errors = []\n",
    "    for error in errors:\n",
    "        current_length = len(error)\n",
    "        if current_length < target_length:\n",
    "            nans_to_append = target_length - current_length\n",
    "            padded_error = np.concatenate([error, [np.nan]*nans_to_append])\n",
    "        elif current_length > target_length:\n",
    "            padded_error = error[:target_length]\n",
    "        else:\n",
    "            padded_error = error\n",
    "        padded_errors.append(padded_error)\n",
    "    return np.array(padded_errors)\n",
    "\n",
    "# Function to plot a dictionary of errors with day names\n",
    "def plot_dict_errors(error_dict, title, target_length=61):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    maelist = []\n",
    "    \n",
    "    # Sort the keys based on day number to ensure chronological order\n",
    "    sorted_keys = sorted(\n",
    "        error_dict.keys(),\n",
    "        key=lambda x: int(x.split('_')[-1])  # Extract the day number for sorting\n",
    "    )\n",
    "    \n",
    "    for key in sorted_keys:\n",
    "        errors = error_dict[key]\n",
    "        unique_lengths = set([len(x) for x in errors])\n",
    "        print(f'Key: {key}, Lengths: {unique_lengths}')\n",
    "        \n",
    "        # Pad errors to target_length\n",
    "        padded_errors = pad_errors(errors, target_length)\n",
    "        \n",
    "        try:\n",
    "            mae_per_timestep = np.nanmean(padded_errors, axis=0)\n",
    "            maelist.append(mae_per_timestep)\n",
    "            day_name = day_mapping.get(key, key)  # Replace key with day name\n",
    "            print(day_mapping[key], mae_per_timestep[0], mae_per_timestep[24], mae_per_timestep[48])\n",
    "            plt.plot([-int(x) for x in ETOT_horizons[:target_length]], mae_per_timestep, marker='o', label=day_name)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Error processing key '{key}': {ve}\")\n",
    "            continue\n",
    "\n",
    "    if maelist:\n",
    "        mean_mae = np.nanmean(maelist, axis=0)\n",
    "        print(f'{mean_mae=}')\n",
    "        plt.plot([-int(x) for x in ETOT_horizons[:target_length]], mean_mae, marker='', label='Mean', color='red')\n",
    "\n",
    "    plt.xlabel('Timestep')\n",
    "    xt = [x for x in range(0,320,20)][::-1]\n",
    "    yt = [x for x in range(0,16,2)]\n",
    "\n",
    "    plt.xticks(xt, fontsize=14)\n",
    "    plt.yticks(yt, fontsize=14)\n",
    "    plt.ylabel('Mean Absolute Error (MAE)')\n",
    "    ax = plt.gca()\n",
    "    ax.invert_xaxis()\n",
    "    plt.legend(title=\"Days of the Week\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(f'{e_wd.keys()=}')\n",
    "plot_dict_errors(e_wd, \"MAE per Timestep - Day\")\n",
    "\n",
    "bucketed_errors = defaultdict(list)\n",
    "\n",
    "for time_key, eees in e_time.items():\n",
    "    time_to_cbas = time_key  # Use time_key as time to CBAS (assuming time_key is in minutes)\n",
    "    bucket = int(time_to_cbas//180 )  *3 # Group into 60 minute intervals\n",
    "    bucketed_errors[bucket].extend(eees)\n",
    "\n",
    "\n",
    "bucket_mapping = {\n",
    "    0: \"0:00-3:00\",\n",
    "    3: \"3:00-6:00\",\n",
    "    6: \"6:00-9:00\",\n",
    "    9: \"9:00-12:00\",\n",
    "    12: \"12:00-15:00\",\n",
    "    15: \"15:00-18:00\",\n",
    "    18: \"18:00-21:00\",\n",
    "    21: \"21:00-0:00\"\n",
    "}\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "print(f'{sorted(bucketed_errors.keys())=}')\n",
    "for bucket, edict in sorted(bucketed_errors.items()):\n",
    "    print(f'{bucket=}')\n",
    "    if bucket in [0, 21]:\n",
    "        continue\n",
    "    print(f'{len(edict)=}')\n",
    "    # print(f'{edic]=}')\n",
    "    edict=pad_errors(edict)\n",
    "    mae_per_timestep = np.nanmean(edict, axis=0)\n",
    "\n",
    "    plt.plot([-int(x) for x in ETOT_horizons[:target_length]], mae_per_timestep, marker='o', label=bucket_mapping[bucket])\n",
    "plt.xlabel('Timestep')\n",
    "xt = [x for x in range(0,320,20)][::-1]\n",
    "yt = [x for x in range(0,16,2)]\n",
    "\n",
    "plt.xticks(xt, fontsize=14)\n",
    "plt.yticks(yt, fontsize=14)\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "ax = plt.gca()\n",
    "ax.invert_xaxis()\n",
    "plt.legend(\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.45, -0.15),\n",
    "    ncol=3,  # One column for clarity\n",
    "    frameon=False,\n",
    "    fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the plot\n",
    "print(f'{actypelist}')\n",
    "data = pd.DataFrame({'Aircraftype': [x.split('actype_')[1] for x in actypelist.keys()], 'MAE': [np.nanmean(x) for x in actypelist.values()], 'flightcount': [len(x) for x in actypelist.values()]})\n",
    "\n",
    "print(f'{data=}')\n",
    "# Number of airports\n",
    "N = len(data)\n",
    "\n",
    "# Compute angle for each bar\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # Complete the loop\n",
    "\n",
    "# Heights of bars\n",
    "heights = data['MAE'].tolist()\n",
    "heights += heights[:1]  # Complete the loop\n",
    "\n",
    "# Initialize the plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Remove labels and ticks\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Set the minimum radius (distance from center where bars start)\n",
    "bottom = 2.0  # Adjust this value to change how far bars start from the center\n",
    "\n",
    "# Add bars\n",
    "bar_width = 2 * np.pi / N * 0.8  # Adjust the 0.8 factor to change bar width (spacing between bars)\n",
    "bars = ax.bar(angles[:-1], heights[:-1], width=bar_width, bottom=bottom, color='salmon', alpha=0.6)\n",
    "\n",
    "# Adjust the maximum radius to ensure all bars and labels fit\n",
    "max_height = bottom + max(heights) + max(heights) * 0.1  # Add some padding\n",
    "ax.set_ylim(0, max_height)\n",
    "\n",
    "# Add labels\n",
    "for i, bar in enumerate(bars):\n",
    "    angle = angles[i]\n",
    "    rotation = np.degrees(angle)\n",
    "    alignment = ''\n",
    "    if rotation >= 270 or rotation <= 90:\n",
    "        alignment = 'left'\n",
    "        rotation = rotation\n",
    "    else:\n",
    "        alignment = 'right'\n",
    "        rotation = rotation + 180\n",
    "    # Position the label at the top of the bar\n",
    "    ax.text(\n",
    "        angle,\n",
    "        bottom + heights[i] + max(heights) * 0.05,  # Adjust position based on 'bottom' and 'heights'\n",
    "        data['Aircraftype'].iloc[i],\n",
    "        ha=alignment,\n",
    "        va='center',\n",
    "        rotation=rotation,\n",
    "        rotation_mode='anchor',\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.title('Circular Barplot of MAE per Airport', y=1.08)\n",
    "plt.show()\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define a threshold for flight count to group small aircraft\n",
    "flight_count_threshold = 50\n",
    "\n",
    "# Group small aircraft under 'Other'\n",
    "data_grouped = data.copy()\n",
    "data_grouped.loc[data_grouped['flightcount'] < flight_count_threshold, 'Aircraftype'] = 'Other'\n",
    "\n",
    "# Aggregate the grouped data by summing flight counts and calculating the average MAE\n",
    "grouped_data = data_grouped.groupby('Aircraftype', as_index=False).agg({\n",
    "    'flightcount': 'sum',  # Sum up flight counts for 'Other'\n",
    "    'MAE': 'mean'          # Use average MAE for 'Other'\n",
    "})\n",
    "\n",
    "# Sort the data for better visualization\n",
    "grouped_data = grouped_data.sort_values(by='flightcount', ascending=False)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar chart for flight count\n",
    "bar1 = ax1.bar(grouped_data['Aircraftype'], grouped_data['flightcount'], \n",
    "               color='gray', alpha=0.6, label='Flight Count')\n",
    "ax1.set_xlabel('Aircraft Type')\n",
    "ax1.set_ylabel('Flight Count', color='gray')\n",
    "ax1.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "# Align grid on the primary y-axis\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Line plot for MAE\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(grouped_data['Aircraftype'], grouped_data['MAE'], \n",
    "         color='red', marker='o', label='Mean Absolute Error (MAE)')\n",
    "ax2.set_ylabel('Mean Absolute Error (MAE)', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "yt = [x for x in range(0,20,2)]\n",
    "ax2.set_yticks(yt)\n",
    "ax2.grid(False)\n",
    "\n",
    "# Add legends\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "# Sort e_adep by the length of the flight list\n",
    "sorted_e_adep = dict(sorted(e_adep.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "\n",
    "# Take the top 25 airports\n",
    "top_e_adep = {k: v for k, v in list(sorted_e_adep.items())[:25]}\n",
    "\n",
    "# Flatten the data for boxplot\n",
    "data = []\n",
    "for airport, flight_errors in top_e_adep.items():\n",
    "    for flight_error in flight_errors:\n",
    "        for timestep_error in flight_error:\n",
    "            data.append({\"Airport\": airport.split('_')[1], \"MAE\": timestep_error})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Customize outlier marker properties\n",
    "flierprops = {\n",
    "    \"marker\": \"o\",\n",
    "    \"markersize\": 0.1,  # Slightly larger markers for better visibility\n",
    "    \"markerfacecolor\": \"red\",  # Highlight outliers in red\n",
    "    \"markeredgecolor\": \"black\",  # Black outline for contrast\n",
    "    \"linewidth\": 0.5,\n",
    "}\n",
    "\n",
    "# Create boxplot\n",
    "plt.figure(figsize=(14, 8))  # Larger figure size for clarity\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x=\"Airport\",\n",
    "    y=\"MAE\",\n",
    "    palette=\"coolwarm\",  # Use a colormap for visual differentiation\n",
    "    linewidth=1.5,  # Make lines slightly thicker for better visibility\n",
    "    showfliers=True,  # Show outliers\n",
    "    flierprops=flierprops,  # Apply custom outlier properties\n",
    "    medianprops={\"color\": \"green\", \"linewidth\": 2},  # Make the median line stand out\n",
    "    boxprops={\"facecolor\": \"none\", \"edgecolor\": \"black\", \"linewidth\": 1.5},  # Transparent boxes with bold edges\n",
    "    whiskerprops={\"color\": \"black\", \"linewidth\": 1.5},  # Bold whiskers\n",
    "    capprops={\"color\": \"black\", \"linewidth\": 1.5},  # Bold caps\n",
    ")\n",
    "\n",
    "# Add a horizontal line at 0 for reference\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Airports\", fontsize=14, labelpad=10)\n",
    "plt.ylabel(\"Error\", fontsize=14, labelpad=10)\n",
    "plt.ylim(-100,100)\n",
    "# Customize tick labels\n",
    "plt.xticks(rotation=90, fontsize=12, ha=\"right\")  # Angled ticks with right alignment for readability\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save high-quality plot\n",
    "plt.savefig(\"mae_per_airport_outliers_boxplot.svg\", format=\"svg\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_airports = 50\n",
    "data_grid = data.iloc[:max_airports].reset_index(drop=True)\n",
    "num_airports = len(data_grid)\n",
    "\n",
    "# Compute circle sizes (adjust scaling factor as needed)\n",
    "max_mae = data_grid['MAE'].max()\n",
    "sizes = data_grid['MAE'] / max_mae * 3000  # Adjust the multiplier to change circle sizes\n",
    "\n",
    "# Normalize MAE values for color mapping\n",
    "norm = mcolors.Normalize(vmin=data_grid['MAE'].min(), vmax=data_grid['MAE'].max())\n",
    "cmap = cm.get_cmap('coolwarm')\n",
    "colors = [cmap(norm(value)) for value in data_grid['MAE']]\n",
    "\n",
    "# Grid dimensions\n",
    "grid_rows = 5\n",
    "grid_cols = 10\n",
    "\n",
    "fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(grid_cols*1.5, grid_rows*1.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < num_airports:\n",
    "        mae_value = data_grid['MAE'].iloc[i]\n",
    "        color = colors[i]\n",
    "        ax.scatter(0.5, 0.5, s=sizes.iloc[i], color=color, alpha=0.6)\n",
    "        ax.text(0.5, 0.5, f\"{mae_value:.1f}\", ha='center', va='center', fontsize=12)\n",
    "        ax.axis('off')  # Remove the box around the circle\n",
    "        ax.set_title(data_grid['Airport'].iloc[i], fontsize=10)\n",
    "    else:\n",
    "        ax.axis('off')  # Hide unused subplots\n",
    "\n",
    "plt.suptitle('MAE per Airport', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnames =data_prep.basenames\n",
    "# Plot feature importance\n",
    "input_size = data_prep.input_size\n",
    "\n",
    "model_trainer = LSTMModelTrainerAttention(data_prep=data_prep, input_size=input_size, model_type='varattention')\n",
    "\n",
    "baseline = model_trainer.compute_baseline_mae(best_model, X_test_tensor[:10000], y_test_tensor[:10000], data_prep.scaler_y, time_horizons)\n",
    "imp_df = model_trainer.compute_permutation_importance(best_model, X_test_tensor[:10000], y_test_tensor[:10000], data_prep.scaler_y, time_horizons, bnames)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def normalize_dataframe_minmax(df, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Normalize the entire DataFrame using MinMaxScaler.\n",
    "    \n",
    "    :param df: pandas DataFrame to normalize\n",
    "    :param feature_range: tuple (min, max) desired range of transformed data\n",
    "    :return: Normalized DataFrame, fitted scaler\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    scaled_array = scaler.fit_transform(df)\n",
    "    scaled_df = pd.DataFrame(scaled_array, columns=df.columns, index=df.index)\n",
    "    return scaled_df, scaler\n",
    "\n",
    "impdf2, scaler = normalize_dataframe_minmax(imp_df)\n",
    "model_trainer.plot_permutation_importance(impdf2)\n",
    "# model_trainer.plot_permutation_importance_with_highlights_interactive(imp_df, ['ko', 'visibility','etodepdelay', 'TOBTdelay', 'TSATdelay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.plot_permutation_importance(imp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_features_line(importance_df, top_n=6, relative=False):\n",
    "    \"\"\"\n",
    "    Plot the permutation feature importance as a line plot for the top N features.\n",
    "\n",
    "    :param importance_df: DataFrame containing feature importance scores\n",
    "    :param top_n: Number of top features to display in the plot\n",
    "    :param relative: If True, plot relative importance percentages instead of absolute values\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Set Seaborn style for professional-looking plots\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Filter numeric columns only\n",
    "    numeric_columns = importance_df.columns[pd.to_numeric(importance_df.columns, errors='coerce').notnull()]\n",
    "    importance_df = importance_df[numeric_columns]\n",
    "\n",
    "    # Calculate total importance and select top N features\n",
    "    importance_df['total_importance'] = importance_df.sum(axis=1)\n",
    "    top_features = importance_df.sort_values('total_importance', ascending=False).head(top_n)\n",
    "    importance_df_top = top_features.drop('total_importance', axis=1)\n",
    "    \n",
    "    # Rename feature names for readability\n",
    "    importance_df_top = importance_df_top.rename(index={\n",
    "        'etodepdelay': 'Flight Plan Delay',\n",
    "        'ko': 'Knock-on Delay',\n",
    "        'modeltyp_ACT': 'Modeltyp ACT',\n",
    "        'fltstate_SI': 'Flightstate SI',\n",
    "        'TSATdelay': 'TSAT Delay'\n",
    "    })\n",
    "    \n",
    "    # Compute relative importance percentages if requested\n",
    "    if relative:\n",
    "        # Compute total importance per timestep\n",
    "        total_importance_per_timestep = importance_df_top.sum(axis=0)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        total_importance_per_timestep[total_importance_per_timestep == 0] = 1e-8\n",
    "\n",
    "        # Normalize each feature's importance by the total importance at each timestep\n",
    "        importance_df_top = importance_df_top.divide(total_importance_per_timestep, axis=1) * 100\n",
    "\n",
    "    # Create the figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    markers = ['o', 's', 'D', '^', 'v', 'P', 'X', '*']\n",
    "\n",
    "    # Plot using Seaborn for better aesthetics\n",
    "    for i,feature in enumerate(importance_df_top.index):\n",
    "        sns.lineplot(\n",
    "            x=importance_df_top.columns.astype(float),\n",
    "            y=importance_df_top.loc[feature],\n",
    "            label=feature,\n",
    "            linewidth=1,  # Thicker lines for academic plots\n",
    "            markersize=8,\n",
    "            # marker = markers[i]\n",
    "        )\n",
    "\n",
    "    # Add labels and grid\n",
    "    ylabel = 'Relative Importance (%)' if relative else 'Feature Importance'\n",
    "    plt.xlabel('Time to ATOT (minutes)', fontsize=16)\n",
    "    plt.ylabel(ylabel, fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    # Place the legend below the plot in two rows\n",
    "    plt.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.45, -0.1),  # Center horizontally and adjust vertical spacing\n",
    "        ncol=3,  # 3 items in the first row\n",
    "        frameon=False,\n",
    "        fontsize=16\n",
    "    )\n",
    "\n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "    # Save the figure with high resolution\n",
    "    plt.savefig(\"top_features_lineplot.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_top_features_area(importance_df, top_n=5):\n",
    "    \"\"\"\n",
    "    Plot the permutation feature importance as a stacked area plot with relative importance percentages\n",
    "    for the top N features.\n",
    "\n",
    "    :param importance_df: DataFrame containing feature importance scores\n",
    "    :param top_n: Number of top features to display in the plot\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Filter numeric columns only\n",
    "    numeric_columns = importance_df.columns[pd.to_numeric(importance_df.columns, errors='coerce').notnull()]\n",
    "    importance_df = importance_df[numeric_columns]\n",
    "\n",
    "    # Calculate total importance and select top N features\n",
    "    importance_df['total_importance'] = importance_df.sum(axis=1)\n",
    "    top_features = importance_df.sort_values('total_importance', ascending=False).head(top_n).index\n",
    "    importance_df_top = importance_df.loc[top_features].drop('total_importance', axis=1)\n",
    "\n",
    "    # Compute total importance per timestep\n",
    "    total_importance_per_timestep = importance_df_top.sum(axis=0)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    total_importance_per_timestep[total_importance_per_timestep == 0] = 1e-8\n",
    "\n",
    "    # Compute relative importance percentages per timestep\n",
    "    importance_df_relative = importance_df_top.divide(total_importance_per_timestep, axis=1) * 100\n",
    "\n",
    "    # Prepare data for stacking\n",
    "    timesteps = importance_df_relative.columns.astype(float)\n",
    "    features = importance_df_relative.index\n",
    "    importance_values = importance_df_relative.values\n",
    "\n",
    "    # Sort timesteps in case they're not in order\n",
    "    sorted_indices = np.argsort(timesteps)\n",
    "    timesteps = timesteps[sorted_indices]\n",
    "    importance_values = importance_values[:, sorted_indices]\n",
    "\n",
    "    # Plot stacked area chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.stackplot(timesteps, importance_values, labels=features)\n",
    "\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Relative Importance (%)')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_features_heatmap(importance_df, top_n=5, relative=False):\n",
    "    \"\"\"\n",
    "    Plot the permutation feature importance as a heatmap for the top N features.\n",
    "\n",
    "    :param importance_df: DataFrame containing feature importance scores\n",
    "    :param top_n: Number of top features to display in the plot\n",
    "    :param relative: If True, plot relative importance percentages instead of absolute values\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Set Seaborn style for professional-looking plots\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Filter numeric columns only\n",
    "    numeric_columns = importance_df.columns[pd.to_numeric(importance_df.columns, errors='coerce').notnull()]\n",
    "    importance_df = importance_df[numeric_columns]\n",
    "\n",
    "    # Calculate total importance and select top N features\n",
    "    importance_df['total_importance'] = importance_df.sum(axis=1)\n",
    "    top_features = importance_df.sort_values('total_importance', ascending=False).head(top_n)\n",
    "    importance_df_top = top_features.drop('total_importance', axis=1)\n",
    "\n",
    "    # Rename feature names for readability\n",
    "    importance_df_top = importance_df_top.rename(index={\n",
    "        'etodepdelay': 'Flight Plan Delay',\n",
    "        'ko': 'Knock-on Delay',\n",
    "        'modeltyp_ACT': 'Modeltyp ACT',\n",
    "        'fltstate_SI': 'Flightstate SI',\n",
    "        'TSATdelay': 'TSAT Delay'\n",
    "    })\n",
    "\n",
    "    # Compute relative importance percentages if requested\n",
    "    if relative:\n",
    "        # Compute total importance per timestep\n",
    "        total_importance_per_timestep = importance_df_top.sum(axis=0)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        total_importance_per_timestep[total_importance_per_timestep == 0] = 1e-8\n",
    "\n",
    "        # Normalize each feature's importance by the total importance at each timestep\n",
    "        importance_df_top = importance_df_top.divide(total_importance_per_timestep, axis=1) * 100\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(\n",
    "        importance_df_top,\n",
    "        cmap=\"coolwarm\",\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        cbar_kws={'label': 'Relative Importance (%)' if relative else 'Feature Importance'},\n",
    "        xticklabels=True,\n",
    "        yticklabels=True\n",
    "    )\n",
    "\n",
    "    # Add labels\n",
    "    plt.xlabel('Time to ATOT (minutes)', fontsize=16)\n",
    "    plt.ylabel('Features', fontsize=16)\n",
    "    plt.xticks(fontsize=12, rotation=45)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Title and layout\n",
    "    plt.title('Feature Importance Over Time', fontsize=18, weight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(\"feature_importance_heatmap.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "numeric_columns = imp_df.columns[pd.to_numeric(imp_df.columns, errors='coerce').notnull()]\n",
    "importance_df = imp_df[numeric_columns]\n",
    "\n",
    "plot_top_features_line(importance_df, relative=True)\n",
    "plot_top_features_area(importance_df)\n",
    "plot_top_features_heatmap(importance_df, top_n=7, relative=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
