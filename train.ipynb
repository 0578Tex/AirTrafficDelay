{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data_prep import DataPreparation, filtering_data\n",
    "from lstm import LSTMModelTrainerAttention, LSTMRollingForecaster\n",
    "from rf import RandomForestTrainer\n",
    "from lgbm import LightGBMTrainer\n",
    "from catboost_trainer import CatBoostTrainer\n",
    "from transformer import TransformerModelTrainer\n",
    "import os\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= r'C:\\Users\\iLabs_6\\Documents\\Tex\\allwithCBAS'\n",
    "modelname = 'extended_df_ATOT-jorn2'\n",
    "modelname = 'extended_df_ATOT'\n",
    "output= r'C:\\Users\\iLabs_6\\Documents\\Tex\\realtimetest3'\n",
    "\n",
    "modelname = 'newbaseline_atot2'\n",
    "# output= r'C:\\Users\\iLabs_6\\Documents\\Tex\\allwithCBAS'\n",
    "# modelname = 'extended_df_ATOT'\n",
    " \n",
    "with  open(os.path.join(output, modelname), 'rb') as f:  \n",
    "    extended_df = pickle.load(f)\n",
    "\n",
    "extended_df['cap_DEP'].fillna(extended_df['cap_DEP'].mean(), inplace=True)\n",
    "extended_df['cap_DES'].fillna(extended_df['cap_DES'].mean(), inplace=True)\n",
    "extended_df = extended_df.loc[:, ~extended_df.columns.str.contains('t_to_eobt', case=False)]\n",
    "extended_df = extended_df.loc[:, ~extended_df.columns.str.contains('t_to_atot', case=False)]\n",
    "# extended_df = extended_df.drop_duplicates()\n",
    "# print(f'{extended_df.describe()=}')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "X, y, colnames = filtering_data(extended_df, airport ='EHAM', save=True)\n",
    "X = X.drop(['ADES_EHAM', 'ADESLong', 'ADESLat', 'cbasentry'], axis =1)\n",
    "# X.describe()\n",
    "\n",
    "# output= r'C:\\Users\\iLabs_6\\Documents\\Tex\\realtimetest3'\n",
    "modelname = 'extended_df_ETOT'\n",
    "modelname = 'newbaseline_etot2'\n",
    "\n",
    "\n",
    "with  open(os.path.join(output, modelname), 'rb') as f:  \n",
    "    extended_real= pickle.load(f)\n",
    "extended_real = extended_real.loc[:, ~extended_real.columns.str.contains('t_to_eobt', case=False)]\n",
    "extended_real = extended_real.loc[:, ~extended_real.columns.str.contains('t_to_atot', case=False)]\n",
    "# extended_real = extended_real.drop_duplicates()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# print(f'{extended_real}')\n",
    "X_r, y_r, colnames = filtering_data(extended_real, airport ='EHAM', save=False)\n",
    "X_r = X_r.drop(['ADES_EHAM', 'ADESLong', 'ADESLat', 'cbasentry'], axis =1)\n",
    "# X.fillna(0, inplace=True)\n",
    "# print(f'xxxxx = {X_r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dform = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "ext = extended_real.assign(EOBT=lambda x: pd.to_datetime(x.EOBT, format=dform)).sort_values(by='EOBT')\n",
    "total_rows = len(ext)\n",
    "last_20_percent_rows = int(total_rows * 0.2)\n",
    "\n",
    "# Select the last 20% of rows\n",
    "last_20_percent = ext.sort_values(by='EOBT').iloc[-last_20_percent_rows:]\n",
    "\n",
    "# Display the description of the last 20%\n",
    "last_20_percent.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep = DataPreparation()\n",
    "\n",
    "\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, time_horizons, cbaslabels = data_prep.fit_transform_data(X, y)\n",
    "X_real, ETOT_horizons, cbaslabels = data_prep.transform_data(X_r)\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_prep.fit_transform_data(X, y, split_ratio=0.8, mode='rf')\n",
    "print(f'{X_train_tensor.shape=}')\n",
    "print(f'{X_test_tensor.shape=}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = LSTMModelTrainerAttention(data_prep=data_prep, input_size=data_prep.input_size, model_type='varattention')\n",
    "print(f'{X_train_tensor.shape=}')\n",
    "print(f'{X_test_tensor.shape=}')\n",
    "# Set data loaders\n",
    "model_trainer.set_data_loaders(data_prep.train_loader, data_prep.test_loader, data_prep.time_steps)\n",
    "\n",
    "# ANDERE SWITCH, afhankelijk van eerste prediction\n",
    "best_model = model_trainer.hyperparameter_search(1)\n",
    "#2707 16 it\n",
    "# Evaluate the best model and plot the results\n",
    "y_pred, y_test = model_trainer.evaluate_and_plot(best_model, X_test_tensor, y_test_tensor, data_prep.scaler_y, time_horizons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_test = model_trainer.evaluate_and_plot(best_model, X_test_tensor, y_test_tensor, data_prep.scaler_y, time_horizons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= r\"C:\\Users\\iLabs_6\\Documents\\Tex\\AirTrafficDelays\\LSTM_Models\"\n",
    "\n",
    "modelname = 'newbaseline-s6'\n",
    "with open(os.path.join(output, modelname), 'wb') as f:  \n",
    "    pickle.dump(best_model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"FlightDataset has been saved successfully. {modelname}\")\n",
    "\n",
    "with  open(os.path.join(output, modelname), 'rb') as f:  \n",
    "    best_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomForestTrainer with desired hyperparameters\n",
    "trainer_rf = RandomForestTrainer(dataprep=data_prep, horizons=data_prep.time_horizons)\n",
    "\n",
    "\n",
    "# # Share the scaler_y between data_prep_rf and trainer_rf\n",
    "trainer_rf.scaler_y = data_prep.scaler_y\n",
    "\n",
    "# # Train the model\n",
    "trainer_rf.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_rf.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_gbm = LightGBMTrainer(dataprep=data_prep,n_estimators=20, max_depth=10, min_samples_split=5, horizons=data_prep.time_horizons)\n",
    "\n",
    "# Share the scaler_y between data_prep_rf and trainer_rf\n",
    "trainer_gbm.scaler_y = data_prep.scaler_y\n",
    "\n",
    "# Train the model\n",
    "trainer_gbm.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_gbm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cat = CatBoostTrainer(dataprep=data_prep, horizons=data_prep.time_horizons)\n",
    "\n",
    "# Share the scaler_y between data_prep_rf and trainer_rf\n",
    "trainer_cat.scaler_y = data_prep.scaler_y\n",
    "\n",
    "# Train the model\n",
    "trainer_cat.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_cat.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import TransformerModelTrainer\n",
    "import torch\n",
    "\n",
    "\n",
    "transformer_trainer = TransformerModelTrainer(\n",
    "    dataprep=data_prep,\n",
    "    horizons=[x for x in range(0,305,5)][::-1],\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    model_folder='transformer_models',\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=32,\n",
    "    num_epochs=10,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "# Train the Transformer models\n",
    "transformer_trainer.train(X_train_tensor, y_train_tensor, X_val=None, y_val_dict=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import TransformerModelTrainer, TransformerRollingForecaster\n",
    "\n",
    "\n",
    "transformer_trainer = TransformerModelTrainer(data_prep.input_size, data_prep, model_type='Transformer', log_dir='runs/transformer')\n",
    "\n",
    "transformer_trainer.set_data_loaders(data_prep.train_loader, data_prep.test_loader, data_prep.time_steps)\n",
    "\n",
    "# ANDERE SWITCH, afhankelijk van eerste prediction\n",
    "best_transformer_model = transformer_trainer.hyperparameter_search(1)\n",
    "#2707 16 it\n",
    "# Evaluate the best model and plot the results\n",
    "y_pred, y_test = transformer_trainer.evaluate_and_plot(best_transformer_model, X_test_tensor, y_test_tensor, data_prep.scaler_y, time_horizons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= r\"C:\\Users\\iLabs_6\\Documents\\Tex\\AirTrafficDelays\\Transformer_Models\"\n",
    "\n",
    "modelname = 'Transformer_nb7_198.pkl'\n",
    "with open(os.path.join(output, modelname), 'wb') as f:  \n",
    "    pickle.dump(best_transformer_model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"FlightDataset has been saved successfully. {modelname}\")\n",
    "\n",
    "with  open(os.path.join(output, modelname), 'rb') as f:  \n",
    "    best_transformer_model = pickle.load(f)\n",
    "\n",
    "y_pred, y_test = transformer_trainer.evaluate_and_plot(best_transformer_model, X_test_tensor, y_test_tensor, data_prep.scaler_y, time_horizons)\n",
    "y_pred-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformer import TransformerModelTrainer, TransformerRollingForecaster\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Initialize lists for errors and predictions\n",
    "abs_ap = []  # Absolute errors for MAE\n",
    "ap = []\n",
    "squared_errors = []  # Squared errors for RMSE\n",
    "y_true = []  # True values\n",
    "y_pred = []  # Predicted values\n",
    "trans_ap = []\n",
    "target_length = 61  # Target length for each error array\n",
    "start_idx, end_idx = -1000, -100  # Index range\n",
    "start_idx, end_idx =int( 0.8*len(y_r)), -1  #all!\n",
    "\n",
    "# start_idx, end_idx = 56455, -1     # Uncomment to use all indices\n",
    "\n",
    "for fnr in tqdm(np.where((y_r <= 130))[0][start_idx:end_idx], desc=\"Processing Transformer Forecasts\"):\n",
    "    # Create rolling forecaster instance\n",
    "    recursive = TransformerRollingForecaster(best_transformer_model, data_prep, X_real[fnr], ETOT_horizons)\n",
    "    pred = recursive.rolling_forecast()\n",
    "\n",
    "    # Compute absolute and squared errors per timestep\n",
    "    absolute_error_per_timestep = np.abs(pred[-target_length:] - y_r[fnr])\n",
    "    squared_error_per_timestep = (pred[-target_length:] - y_r[fnr]) ** 2\n",
    "    error_per_timestep = (pred[-target_length:] - y_r[fnr])\n",
    "\n",
    "    # Ensure each error array has `target_length` elements by padding with NaNs if needed\n",
    "    if len(absolute_error_per_timestep) < target_length:\n",
    "        pad_length = target_length - len(absolute_error_per_timestep)\n",
    "\n",
    "        padded_abs_error = np.pad(absolute_error_per_timestep, (0, pad_length), constant_values=np.nan)\n",
    "        padded_error = np.pad(error_per_timestep, (0, target_length - len(error_per_timestep)), constant_values=np.nan)\n",
    "        \n",
    "        padded_sq_error = np.pad(squared_error_per_timestep, (0, pad_length), constant_values=np.nan)\n",
    "        abs_ap.append(padded_abs_error)\n",
    "        ap.append(padded_error)\n",
    "\n",
    "        squared_errors.append(padded_sq_error)\n",
    "    else:\n",
    "        abs_ap.append(absolute_error_per_timestep)\n",
    "        squared_errors.append(squared_error_per_timestep)\n",
    "        ap.append(error_per_timestep)\n",
    "\n",
    "\n",
    "    # Handle y_true and y_pred for R² calculation\n",
    "    true_values = y_r[fnr] * np.ones(target_length)  # Repeat the true value\n",
    "    predicted_values = pred[-target_length:]\n",
    "\n",
    "    if len(predicted_values) < target_length:\n",
    "        pad_length = target_length - len(predicted_values)\n",
    "        y_true.append(true_values)\n",
    "        y_pred.append(np.pad(predicted_values, (0, pad_length), constant_values=np.nan))\n",
    "    else:\n",
    "        y_true.append(true_values)\n",
    "        y_pred.append(predicted_values)\n",
    "\n",
    "# Convert lists to NumPy arrays for easier manipulation\n",
    "abs_ap = np.array(abs_ap)  # Shape: (num_samples, target_length)\n",
    "squared_errors = np.array(squared_errors)  # Shape: (num_samples, target_length)\n",
    "y_true = np.array(y_true)  # Shape: (num_samples, target_length)\n",
    "y_pred = np.array(y_pred)  # Shape: (num_samples, target_length)\n",
    "\n",
    "# Compute per-timestep metrics\n",
    "mae_per_timestep = np.nanmean(abs_ap[-61:], axis=0)\n",
    "rmse_per_timestep = np.sqrt(np.nanmean(squared_errors[-61:], axis=0))\n",
    "std_per_timestep = np.nanstd(abs_ap[-61:], axis=0)\n",
    "\n",
    "# Compute overall metrics\n",
    "overall_mae = np.nanmean(abs_ap)\n",
    "overall_rmse = np.sqrt(np.nanmean(squared_errors))\n",
    "\n",
    "# Flatten arrays for overall R² calculation\n",
    "flat_y_true = y_true.flatten()\n",
    "flat_y_pred = y_pred.flatten()\n",
    "valid_mask = ~np.isnan(flat_y_true) & ~np.isnan(flat_y_pred)\n",
    "\n",
    "overall_std = np.nanstd(abs_ap)\n",
    "\n",
    "\n",
    "if np.any(valid_mask):\n",
    "    overall_r2 = r2_score(flat_y_true[-61:], flat_y_pred[-61:])\n",
    "else:\n",
    "    overall_r2 = np.nan\n",
    "\n",
    "# Print raw values\n",
    "print(\"Raw True Values (y_true):\", y_true)\n",
    "print(\"Raw Predictions (y_pred):\", y_pred)\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"Overall MAE: {overall_mae:.4f}\")\n",
    "print(f\"Overall RMSE: {overall_rmse:.4f}\")\n",
    "print(f\"Overall R²: {overall_r2:.4f}\")\n",
    "print(f\"Overall Std Deviation of MAE: {overall_std:.4f}\")\n",
    "\n",
    "# Plot MAE and RMSE per timestep\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ETOT_horizons[:target_length], mae_per_timestep, marker='o', label='MAE')\n",
    "plt.plot(ETOT_horizons[:target_length], rmse_per_timestep, marker='x', label='RMSE')\n",
    "plt.xlabel('Timestep')\n",
    "plt.xticks(ETOT_horizons[:target_length], rotation='vertical')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Transformer: MAE and RMSE for Each Timestep')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot mean error with standard deviation\n",
    "mean_error_per_timestep = np.nanmean(y_true - y_pred, axis=0)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ETOT_horizons[:target_length], mean_error_per_timestep, marker='o', label='Mean Error')\n",
    "plt.fill_between(\n",
    "    ETOT_horizons[:target_length],\n",
    "    mean_error_per_timestep - std_per_timestep,\n",
    "    mean_error_per_timestep + std_per_timestep,\n",
    "    color='b',\n",
    "    alpha=0.2,\n",
    "    label='±1 Std Dev'\n",
    ")\n",
    "plt.xlabel('Timestep')\n",
    "plt.xticks(ETOT_horizons[:target_length], rotation='vertical')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.title('Transformer: Mean Error for Each Timestep with Standard Deviation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
